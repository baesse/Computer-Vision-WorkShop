{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neste lab vamos utilizar uma Rede Multilayer Perceptron (MLP) para tentar classificar dígitos do dataset MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando pacotes\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando o dataset MNIST (http://yann.lecun.com/exdb/mnist/)\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "old_v = tf.logging.get_verbosity()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veja abaixo alguns exemplos do dataset MNIST\n",
    "from IPython.display import Image\n",
    "Image(filename='08_mnist.jpg') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos definir alguns parâmetros para o treinamento da Rede Neural:\n",
    "    \n",
    "**learning_rate** - Taxa de aprendizado. Quanto menor, mais demorado será o treinamento, mas poderá encontrar os melhores valores\n",
    "    \n",
    "**training_epochs** - Quantas épocas de treinamento. O treinamento melhora com o avanço das épocas, entretanto se for muito grande poderá provocar o overfitting. Se for muito baixo, pode ocorrer o underfitting\n",
    "\n",
    "**batch_size** - Quantidade da amostra utilizada em cada época de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parâmetros do treinameno\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O próximo passo será definir a estrutura da Rede Neural (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Camadas\n",
    "n_hidden_1 = 256 # primeira camada oculta\n",
    "n_hidden_2 = 256 # segunda camada oculta\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos definir dois tensores, um para os dados de entrada e outro para o de saída"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja que os dados de entrada consistem em um Array de duas dimensoes **[None, n_input]**\n",
    "\n",
    "A primeira **None** diz respeito aos dados de entrada, que serão fornecidos depois. \n",
    "\n",
    "A segunda **n_input** foi fixada em 784, que corresponde à sequência de pixels de uma imagem MNIST de 28x28 pixels\n",
    "\n",
    "Ou seja, neste método de classificação não está nos importanto o fator proximidade de pixels, mas apenas os pixels um depois do outro até terminar o arquivo da imagem.\n",
    "\n",
    "Sabemos que esta não é a melhor forma de classificar imagens, mas é um início. Depois veremos e utilizaremos as CNNs\n",
    "\n",
    "A segunda entrada é no formato **[None, n_classes]**\n",
    "\n",
    "**None** diz respeito aos dados de saída esperada, que serão fornecidos depois. \n",
    "\n",
    "**n_classes** foi fixada em 10, visto que são apenas os caracteres numéricos de '0' a '9' que fazem parte desse dataset\n",
    "\n",
    "Em seguida vamos criar uma função para facilitar a criação da Rede Neural\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para criar a MLP\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Camada oculta com ativação RELU \n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Camada oculta com ativação RELU \n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Output layer com ativação linear\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos definir as variáveis que devem ser encontradas durante o treinamento da Rede Neural, após a passagem dos dados de entrada e saída previstos\n",
    "\n",
    "Vamos definir aqui os dados de **peso** e **bias** para cada Perceptron (Neurônio) da rede MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceba que acima definimos valores aleatórios para essas variáveis, pois precisam iniciar o treinamento com algum valor aleatório atribuído.\n",
    "\n",
    "Com isso, a rede neural terá um desempenho ruim nas primeiras épocas, e vai melhorando com o tempo após aprender com os dados de treino\n",
    "\n",
    "Em seguida vamos utlizar a função criada anteriormente para criar a MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rede = multilayer_perceptron(x, weights, biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como você já sabe, precisamos agora definir uma função de custo para dizer quão longe uma previsão está longe do valor correto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost Function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = rede, labels = y))\n",
    "tf.summary.scalar('cost', cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Além disso precisamos de uma função de otimização, que vai encontrar os valores de peso e bias que minimizam a função de custo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pronto! Nosso blueprint da rede neural já está pronto:\n",
    "- Definimos os valores de treinamento X e Y\n",
    "- Definimos as variáveis a serem encontrada\n",
    "- Definimos a estrutura da Rede Neural Multilayer Perceptron\n",
    "- Definimos uma função de custo\n",
    "- Definimos uma função de otimização\n",
    "\n",
    "Agora podemos executar o modelo com o TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialziando as variáveis\n",
    "init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando variáveis para plotar o gráfico ao final\n",
    "avg_cost_set = []\n",
    "epoch_set = []\n",
    "# Sessão\n",
    "with tf.Session() as sess:\n",
    "    # Inicializa as variaveis\n",
    "    sess.run(init)\n",
    "    merged = tf.summary.merge_all()\n",
    "    now = datetime.now()\n",
    "    writer = tf.summary.FileWriter('logs_08/'+now.strftime(\"%Y%m%d-%H%M%S\"), sess.graph)\n",
    "    # Ciclo de Treinamento\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop por todos os batches\n",
    "        for i in range(total_batch):\n",
    "            # Vamos utilizar uma função do próprio MNIST para buscar os N registros a serem utilizados em uma época\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Executa a otimização com o método run() do TF , passando os dados de treinamento X e Y conhecidos\n",
    "            # Vejá que ignoramos o resultado do otimizador mas obtemos o resultado da função de custo \n",
    "            train_summary, _, c = sess.run([merged, optimizer, cost], feed_dict = {x: batch_x, y: batch_y})\n",
    "            # Computando a média do custo\n",
    "            avg_cost += c / total_batch\n",
    "        writer.add_summary(train_summary, epoch)\n",
    "        avg_cost_set.append(avg_cost)\n",
    "        epoch_set.append(epoch)\n",
    "        # Display logs por epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print (\"Epoca:\", '%04d' % (epoch+1), \"Custo = \", \"{:.9f}\".format(avg_cost))\n",
    "    print (\"Treinamento Concluído com \",training_epochs,\" épocas!\")\n",
    "\n",
    "    # Vamos plotar ao final o gráfico de custo por época de treinamento\n",
    "    plt.plot(epoch_set,avg_cost_set, '-', label = 'Custo')\n",
    "    plt.ylabel('Custo')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Testando o modelo\n",
    "    # Primeiro definimos uma para comparar o resultado da rede com os valores de y já conhecidos \n",
    "    correct_prediction = tf.equal(tf.argmax(rede, 1), tf.argmax(y, 1))\n",
    "    # Em seguida calculando a acurácia\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print (\"Acurácia final: \", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abra o **TensorBoard** utilizando o comando abaixo e veja os gráficos de custo e acurácia gerados:\n",
    "    \n",
    "python -m tensorboard.main --logdir=\"logs_08\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício:\n",
    "\n",
    "Modifique os parâmetros de treinamento e rode novamente a rede para ver se consegue melhorar o resultado\n",
    "\n",
    "Registre em uma planilha Excel os valores utilizados para cada treinamento e os valores de **custo** e **acurácia** final obtidos\n",
    "\n",
    "## Responda:\n",
    "\n",
    "- Qual a influência do learning_rate?\n",
    "- Qual a influência do batch_size?\n",
    "- Qual a influência da quantidade de épocas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
